{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICI ca test de recup des infos sur nba.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver #/!\\ version :4.5.0\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a constant for the url\n",
    "NBA_PLAYERS_STATS_URL = 'https://www.nba.com/stats/alltime-leaders'\n",
    "NBA_PLAYERS_INFO_URL = 'https://www.nba.com/players'\n",
    "NBA_PLAYERS_AGILITY_URL = 'https://www.nba.com/stats/draft/combine-strength-agility'\n",
    "\n",
    "# path to the file containing the chrome driver\n",
    "CHROME_DRIVER_PATH = \"./chromedriver.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Browser functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the browser.\n",
    "def startBrowser():\n",
    "    s=Service(CHROME_DRIVER_PATH)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless\")\n",
    "    return  webdriver.Chrome(service=s, options=options)\n",
    "\n",
    "# Checking if the browser is closed. // used to check if the user has closed the browser manually\n",
    "def isBrowserClosed(browser):\n",
    "    isbrowserClosed = False\n",
    "    try:\n",
    "        webdriver.title\n",
    "    except:\n",
    "        isbrowserClosed = True\n",
    "    return isbrowserClosed\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert height in df to cm\n",
    "def convertHeightToCm(height):\n",
    "    height = re.findall(r'\\d+-\\d+', height)\n",
    "    if len(height) == 0:\n",
    "        return 0\n",
    "\n",
    "    feet = int(height[0].split('-')[0])\n",
    "    inches = int(height[0].split('-')[1])\n",
    "    return (feet * 12 + inches) * 2.54\n",
    "\n",
    "#convert weight in df to kg\n",
    "def convertWeightToKg(weight):\n",
    "    weight = re.findall(r'\\d+', weight)\n",
    "    if(len(weight) == 0):\n",
    "        return 0\n",
    "    return float(weight[0]) * 0.453592\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scapping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above code is getting the data from the URL and returning the dataframe.\n",
    "def getDataFromURL(URL):\n",
    "    browser = startBrowser()\n",
    "    #open the url\n",
    "    browser.get(URL)\n",
    "\n",
    "    time.sleep(1)\n",
    "    # accept cookies\n",
    "    browser.find_element(By.CSS_SELECTOR, \"button#onetrust-accept-btn-handler\").click()\n",
    "    time.sleep(5)\n",
    "\n",
    "    if URL == NBA_PLAYERS_INFO_URL: #this is to get infos from all time players\n",
    "        #click on the button to show all players\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"__next\"]/div[2]/div[2]/main/div[2]/section/div/div[2]/div[1]/div[6]').click()\n",
    "        time.sleep(1)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    while browser.find_element(By.CSS_SELECTOR, \"button[title^='Next Page Button']\").is_enabled():\n",
    "        html = browser.page_source\n",
    "        data, headers = getDataFromHTML(html)\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame(data, columns=headers)], ignore_index=True)\n",
    "\n",
    "        print(\".\", end = '')\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                browser.find_element(By.CSS_SELECTOR, \"button[title^='Next Page Button']\").click()\n",
    "                break\n",
    "            except:\n",
    "                if(isBrowserClosed(browser)):\n",
    "                    return df\n",
    "                else:\n",
    "                    print('not yet clickable')\n",
    "                    continue\n",
    "\n",
    "    html = browser.page_source\n",
    "    data, headers = getDataFromHTML(html)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(data, columns=headers)], ignore_index=True)\n",
    "\n",
    "    print(\".\", end = '')\n",
    "\n",
    "    #close the browser\n",
    "    browser.quit()\n",
    "    #return the dataframe\n",
    "    return df\n",
    "\n",
    "def getDataAgilityFromURL(URL, startYear, endYear):\n",
    "    browser = startBrowser()\n",
    "    #open the url\n",
    "    browser.get(URL)\n",
    "\n",
    "    time.sleep(1)\n",
    "    # accept cookies\n",
    "    browser.find_element(By.CSS_SELECTOR, \"button#onetrust-accept-btn-handler\").click()\n",
    "    time.sleep(5)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    currentYear = startYear\n",
    "\n",
    "    while currentYear <= endYear:\n",
    "        #select the year\n",
    "        # send key arrow down\n",
    "        browser.find_element(By.CSS_SELECTOR, \"select.DropDown_select__4pIg9\").send_keys(Keys.ARROW_DOWN)\n",
    "        time.sleep(1)\n",
    "\n",
    "        html = browser.page_source\n",
    "        data, headers = getDataFromHTML(html)\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame(data, columns=headers)], ignore_index=True)\n",
    "\n",
    "        print(\".\", end = '')\n",
    "\n",
    "        currentYear += 1\n",
    "\n",
    "    #close the browser\n",
    "    browser.quit()\n",
    "    #return the dataframe\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#Parsing the html and returning the data and headers.\n",
    "def getDataFromHTML(html):\n",
    "    #parse the html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #find the table with class Crom_table__p1iZz or players-list\n",
    "    table = soup.find('table', attrs = {'class' : ['Crom_table__p1iZz','players-list']})\n",
    "    #get the table headers\n",
    "    headers = [header.text for header in table.findAll('th', attrs = {'hidden': None})]\n",
    "    #get the table rows\n",
    "    rows = table.find_all('tr')\n",
    "    #get the table data\n",
    "    data = [[td.text for td in rows[i].find_all('td')] for i in range(len(rows))]\n",
    "    data = [row for row in data if row != []]#they is an empty at the start idk why but yes\n",
    "\n",
    "    return data, headers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get stats from all players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of the scrapping\n",
      "................................................................................................\n",
      "end of the scrapping\n",
      "\n",
      "\n",
      "saving the dataframe to a csv file...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"start of the scrapping\")\n",
    "df = getDataFromURL(NBA_PLAYERS_STATS_URL)\n",
    "print(\"\\nend of the scrapping\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"saving the dataframe to a csv file...\")\n",
    "df.to_csv('nbaPlayersAllTimesStatsData.csv', index = False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataframe has 4771 rows and 23 columns\n"
     ]
    }
   ],
   "source": [
    "#get nb rows and nb columns\n",
    "print(\"the dataframe has\", df.shape[0], \"rows and\", df.shape[1], \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Info from all players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of the scrapping\n",
      ".................................................................................................\n",
      "end of the scrapping\n",
      "\n",
      "\n",
      "Converting height and weight to cm and kg...done\n",
      "\n",
      "\n",
      "saving the dataframe to a csv file...done\n"
     ]
    }
   ],
   "source": [
    "print(\"start of the scrapping\")\n",
    "df = getDataFromURL(NBA_PLAYERS_INFO_URL)\n",
    "print(\"\\nend of the scrapping\\n\\n\")\n",
    "\n",
    "print(\"Converting height and weight to cm and kg...\", end = '')\n",
    "#convert height in df to cm\n",
    "df['Height'] = df['Height'].apply(convertHeightToCm)\n",
    "# convert weight in df to kg\n",
    "df['Weight'] = df['Weight'].apply(convertWeightToKg)\n",
    "print(\"done\\n\\n\")\n",
    "\n",
    "print(\"saving the dataframe to a csv file...\", end = '')\n",
    "df.to_csv('nbaPlayersAllTimesInfo.csv', index = False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataframe has 4804 rows and 8 columns\n"
     ]
    }
   ],
   "source": [
    "#get nb rows and nb columns\n",
    "print(\"the dataframe has\", df.shape[0], \"rows and\", df.shape[1], \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get player Agility infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of the scrapping\n",
      "........................\n",
      "end of the scrapping\n",
      "\n",
      "\n",
      "saving the dataframe to a csv file...done\n"
     ]
    }
   ],
   "source": [
    "print(\"start of the scrapping\")\n",
    "df = getDataAgilityFromURL(NBA_PLAYERS_AGILITY_URL, 2000, 2023)\n",
    "print(\"\\nend of the scrapping\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"saving the dataframe to a csv file...\", end = '')\n",
    "df.to_csv('nbaPlayersAllTimesAgilityData.csv', index = False)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataframe has 1606 rows and 8 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"the dataframe has\", df.shape[0], \"rows and\", df.shape[1], \"columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
